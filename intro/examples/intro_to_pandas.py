# -*- coding: utf-8 -*-
"""AIPy 1D - Pandas

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gqSODfFxhIMZsLJnCGGvsyjVpGkw8fWr

# Introduction to Pandas

## Data Structures: Series
"""

import pandas as pd

# Creating a Series
s1 = pd.Series([1, 2, 3, 4, 5])
s2 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])
s3 = pd.Series({'a': 1, 'b': 2, 'c': 3})

# Series attributes
print(s1.values)  # Array of values
print(s1.index)   # Index object
print(s1.dtype)   # Data type

"""## Data Structures: DataFrame"""

import numpy as np

# Creating a DataFrame
df1 = pd.DataFrame({
    'name': ['John', 'Anna', 'Peter'],
    'age': [28, 22, 35],
    'city': ['New York', 'Paris', 'London']
})

# From a list of dictionaries
data = [
    {'name': 'John', 'age': 28},
    {'name': 'Anna', 'age': 22}
]
df2 = pd.DataFrame(data)

# From NumPy array
arr = np.random.rand(3, 3)
df3 = pd.DataFrame(arr, columns=['A', 'B', 'C'])

"""## Reading Data"""

# Reading from various sources

# Note that these require the specified resources to be available

# df = pd.read_csv('data.csv')
# df = pd.read_excel('data.xlsx')
# df = pd.read_sql('SELECT * FROM table', connection)
# df = pd.read_json('data.json')

"""## Data frame operations"""

# Create a sample DataFrame
df = pd.DataFrame({
    'Name': ['John', 'Alice', 'Bob', 'Emma', 'Mike'],
    'Age': [25, 30, 35, 28, 42],
    'Salary': [50000, 60000, 75000, 58000, 82000],
    'Department': ['IT', 'HR', 'Sales', 'IT', 'Management'],
    'Start_Date': ['2023-01-15', '2022-06-01', '2021-09-30', '2023-03-01', '2020-12-15']
})

# Convert Start_Date to datetime type
df['Start_Date'] = pd.to_datetime(df['Start_Date'])

# Basic DataFrame operations
print(df.head())        # First 5 rows
print("------")
print(df.info())        # DataFrame info
print("------")
print(df.describe())    # Statistical summary
print("------")
print(df.columns)       # Column names
print("------")
print(df.shape)         # Dimensions

"""## Data Selection and Indexing"""

df = pd.DataFrame({
    'name': ['John', 'Alice', 'Bob', 'Emma', 'John', 'Mike'],
    'userid' : ['JDoe', 'ASmith', 'BJohnson', 'EWilson', 'JBrown', 'MDavis'],
    'age': [25, np.nan, 35, 28, 25, 42],
    'salary': [50000, 60000, np.nan, 58000, 50000, 82000],
    'department': ['IT', 'HR', None, 'IT', 'IT', 'Management'],
    'start_date': ['2023-01-15', '2022-06-01', '2021-09-30', np.nan, '2023-01-15', '2020-12-15']
})
df.set_index('userid', inplace=True)

# Column selection
salaries = df['salary']
subset = df[['department', 'age']]
print(salaries)
print(subset)

# Row selection
first_row = df.iloc[0]      # Integer position
named_row = df.loc['BJohnson']  # Named index
filtered = df[df['age'] > 25] # Boolean indexing
print(first_row)
print(filtered)

# Combined selection
value = df.loc['ASmith', 'start_date']   # Value at row 0, column 'start_date'
subset = df.iloc[0:3, 0:2]  # First 3 rows, first 2 columns
print(value)
print(subset)



"""# Handling Missing Data

"""

print("Missing values:")
print(df.isnull().sum())

# Handling missing values
df_dropped = df.dropna()           # Remove rows with any missing values
df_filled_zero = df.fillna(0)      # Fill missing values with 0
df_filled_ffill = df.ffill()       # Forward fill

# Verify numeric columns exist and calculate mean
numeric_cols = ['Age', 'Salary']
print("\nAvailable columns:", df.columns.tolist())
if all(col in df.columns for col in numeric_cols):
    df_filled_mean = df.fillna(df[numeric_cols].mean())
    print("\nDataFrame with mean-filled values:")
    print(df_filled_mean)
else:
    print("Error: One or more numeric columns not found")

# Dropping duplicates
df_unique = df.drop_duplicates()
print("\nAfter dropping duplicates:")
print(df_unique)

"""# Data Transformation"""

# String operations
df['name'] = df['name'].str.upper()           # Change names to uppercase
df['name_length'] = df['name'].str.len()      # Calculate length of names

# Categorical encoding
df['department'] = pd.Categorical(df['department'])
dummy_vars = pd.get_dummies(df['department'])        # Create dummy variables

# Applying functions
df['age_squared'] = df['age'].apply(lambda x: x**2 if pd.notnull(x) else np.nan)
df['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 50, 65, 100])

# Print results to see the changes
print("Modified DataFrame:")
print(df)
print("\nDummy variables:")
print(dummy_vars)

"""# Grouping and Aggregation"""

df['city'] = ['New York', 'Boston', 'New York', 'Boston', 'New York', 'Chicago']

# Group by operations
by_city = df.groupby('city')
city_stats = by_city.agg({
    'age': ['mean', 'min', 'max'],
    'salary': 'mean'
})

# Pivot tables
pivot = df.pivot_table(
    values='salary',
    index='city',
    columns='department',
    aggfunc='mean'
)

# Rolling calculations
df['rolling_mean'] = df['salary'].rolling(window=3).mean()

# Print results
print("City Statistics:")
print(city_stats)
print("\nPivot Table:")
print(pivot)
print("\nDataFrame with Rolling Mean:")
print(df)

"""# Merging & Joining"""

# Create sample DataFrames
df3 = pd.DataFrame({
    'id': [1, 2, 3, 4],
    'name': ['Alice', 'Bob', 'Charlie', 'David'],
    'age': [25, 30, 35, 40]
})

df4 = pd.DataFrame({
    'id': [2, 3, 5, 6],
    'city': ['New York', 'London', 'Paris', 'Tokyo'],
    'salary': [50000, 60000, 75000, 80000]
})

# 1. Merge: Combines DataFrames based on a common column (inner join by default)
merged = pd.merge(df3, df4, on='id')
print("Merged result:")
print(merged)
print("\n")

# 2. Join: Combines DataFrames based on index or a key column
# Note: For join() to work properly with 'id', we should set it as index first
df3_indexed = df3.set_index('id')
df4_indexed = df4.set_index('id')
joined = df3_indexed.join(df4_indexed, on='id').reset_index()
print("Joined result:")
print(joined)
print("\n")

# 3. Concatenate: Stacks DataFrames vertically or horizontally
concatenated = pd.concat([df3, df4], axis=0, ignore_index=True)
print("Concatenated result:")
print(concatenated)

"""## Feature Engineering"""

# Create a sample DataFrame
np.random.seed(42)  # For reproducibility
dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
categories = ['A', 'B', 'C']

data = {
    'Start_Date': np.random.choice(dates, size=100),
    'category': np.random.choice(categories, size=100),
    'value': np.random.normal(100, 15, size=100)
}

df = pd.DataFrame(data)

# Sort the DataFrame by date for better visualization
df = df.sort_values('Start_Date')

# Display first few rows
print("Original DataFrame:")
print(df.head())
print("\n")

# 1. Time-based features
print("Adding time-based features...")
df['year'] = df['Start_Date'].dt.year
df['month'] = df['Start_Date'].dt.month
df['day_of_week'] = df['Start_Date'].dt.dayofweek  # 0 = Monday, 6 = Sunday

# 2. Statistical features
print("Adding statistical features...")
df['z_score'] = (df['value'] - df['value'].mean()) / df['value'].std()
df['pct_rank'] = df['value'].rank(pct=True)

# 3. Window features
print("Adding window features...")
df['rolling_mean'] = df.groupby('category')['value'].transform(
    lambda x: x.rolling(window=7, min_periods=1).mean()
)

# Display the modified DataFrame
print("\nModified DataFrame with new features:")
print(df.head())
print("\n")

# Show some basic info about the new features
print("Sample of new features:")
print(df[['Start_Date', 'year', 'month', 'day_of_week',
         'value', 'z_score', 'pct_rank', 'rolling_mean']].head())

df.head()

"""## Best Practices: Use Proper Data Types"""

# Convert to Pandas category type  when there are multiple repeated values
# This is more memory efficient than storing strings and enables better
#   performance when sorting & grouping

df['category'] = df['category'].astype('category')

# Use appropriate numeric types
df['month'] = df['month'].astype('int32')

"""## Best Practices: Vectorized Ops, Not Loops
For a DataFrame with 1 million rows, the loop version could take seconds or minutes, while the vectorized version may complete in just milliseconds
"""

# Bad (slow):
for i in range(len(df)):
    df.loc[i, 'new_col'] = df.loc[i, 'old_col'] * 2

# Good (fast):
df['new_col'] = df['old_col'] * 2
df.head()

"""## Best Practices: Chain Methods Efficiently"""

# Use method chaining
result = (df.groupby('category')
           .agg({'value': 'mean'})
           .reset_index()
           .rename(columns={'value': 'mean_value'}))

df.head()