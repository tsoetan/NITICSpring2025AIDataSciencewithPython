{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aF4e3qUYhnM0",
        "outputId": "1b7166e2-5b0c-490c-db91-fc56a852f091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found the following CSV files:\n",
            "  - additional-quiz-results.csv\n",
            "  - student-records.csv\n",
            "  - course-assignments.csv\n",
            "  - login-activity.csv\n",
            "  - assessment-scores.csv\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# List all CSV files in the directory\n",
        "data_dir = './hungergames/'\n",
        "data_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
        "\n",
        "# Print out the files we'll be working with\n",
        "print(\"Found the following CSV files:\")\n",
        "for file in data_files:\n",
        "    print(f\"  - {os.path.basename(file)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for file in data_files:\n",
        "  print(file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb8VWxwVO9_-",
        "outputId": "ef8d97b0-f9c5-4cd9-88b4-073e9842c7a4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./hungergames/additional-quiz-results.csv\n",
            "./hungergames/student-records.csv\n",
            "./hungergames/course-assignments.csv\n",
            "./hungergames/login-activity.csv\n",
            "./hungergames/assessment-scores.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discovery"
      ],
      "metadata": {
        "id": "0S_wl4H0jHok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the data sources\n",
        "for file in data_files:\n",
        "    df = pd.read_csv(file)\n",
        "    print(f\"File: {file}\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
        "    print(\"---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EayKI-CejJEb",
        "outputId": "7c24ae96-125d-469e-c7f2-1fe0872b62ba"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: ./hungergames/additional-quiz-results.csv\n",
            "Shape: (32, 5)\n",
            "Columns: ['Name', 'Quiz ID', 'Time Taken', 'Mark', 'Status']\n",
            "Missing values:\n",
            "Name          0\n",
            "Quiz ID       0\n",
            "Time Taken    1\n",
            "Mark          0\n",
            "Status        0\n",
            "dtype: int64\n",
            "---\n",
            "File: ./hungergames/student-records.csv\n",
            "Shape: (16, 6)\n",
            "Columns: ['student_id', 'first_name', 'last_name', 'district', 'registration_date', 'status']\n",
            "Missing values:\n",
            "student_id           1\n",
            "first_name           0\n",
            "last_name            6\n",
            "district             0\n",
            "registration_date    0\n",
            "status               1\n",
            "dtype: int64\n",
            "---\n",
            "File: ./hungergames/course-assignments.csv\n",
            "Shape: (9, 6)\n",
            "Columns: ['assignment_id', 'title', 'due_date', 'max_points', 'weight', 'category']\n",
            "Missing values:\n",
            "assignment_id    0\n",
            "title            0\n",
            "due_date         0\n",
            "max_points       0\n",
            "weight           0\n",
            "category         0\n",
            "dtype: int64\n",
            "---\n",
            "File: ./hungergames/login-activity.csv\n",
            "Shape: (16, 6)\n",
            "Columns: ['id', 'first_login', 'last_login', 'logins_count', 'avg_session_minutes', 'device_type']\n",
            "Missing values:\n",
            "id                     0\n",
            "first_login            0\n",
            "last_login             0\n",
            "logins_count           0\n",
            "avg_session_minutes    0\n",
            "device_type            0\n",
            "dtype: int64\n",
            "---\n",
            "File: ./hungergames/assessment-scores.csv\n",
            "Shape: (16, 5)\n",
            "Columns: ['stu_id', 'assignment_id', 'score', 'submission_time', 'feedback']\n",
            "Missing values:\n",
            "stu_id             0\n",
            "assignment_id      0\n",
            "score              1\n",
            "submission_time    1\n",
            "feedback           1\n",
            "dtype: int64\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structuring"
      ],
      "metadata": {
        "id": "ZmBMSzkjjLLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize column names\n",
        "def standardize_columns(df):\n",
        "    return df.rename(columns={\n",
        "        'student_id': 'student_id',\n",
        "        'stu_id': 'student_id',\n",
        "        'id': 'student_id',\n",
        "        'grade': 'score',\n",
        "        'mark': 'score',\n",
        "        'points': 'score'\n",
        "    })\n",
        "\n",
        "# Apply to all dataframes\n",
        "for df in dfs:\n",
        "  print(\"Before:\", df.columns.tolist())\n",
        "\n",
        "dfs = [standardize_columns(pd.read_csv(file)) for file in data_files]\n",
        "for df in dfs:\n",
        "    print(\"After:\", df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "xd-iNQ0YjMmk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7838e1e7-7a4a-4c7d-ef9f-f7d37414e59d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: ['Name', 'Quiz ID', 'Time Taken', 'Mark', 'Status']\n",
            "Before: ['student_id', 'first_name', 'last_name', 'district', 'registration_date', 'status']\n",
            "Before: ['assignment_id', 'title', 'due_date', 'max_points', 'weight', 'category']\n",
            "Before: ['student_id', 'first_login', 'last_login', 'logins_count', 'avg_session_minutes', 'device_type']\n",
            "Before: ['student_id', 'assignment_id', 'score', 'submission_time', 'feedback']\n",
            "After: ['Name', 'Quiz ID', 'Time Taken', 'Mark', 'Status']\n",
            "After: ['student_id', 'first_name', 'last_name', 'district', 'registration_date', 'status']\n",
            "After: ['assignment_id', 'title', 'due_date', 'max_points', 'weight', 'category']\n",
            "After: ['student_id', 'first_login', 'last_login', 'logins_count', 'avg_session_minutes', 'device_type']\n",
            "After: ['student_id', 'assignment_id', 'score', 'submission_time', 'feedback']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning"
      ],
      "metadata": {
        "id": "avOfjA5gjRBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for df in dfs:\n",
        "    print(\"After:\", df.columns.tolist())\n",
        "for df in dfs:\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    print(df.head(3))\n",
        "    print(\"---\")\n",
        "\n",
        "# Handle student ID inconsistencies\n",
        "def clean_student_id(df):\n",
        "    # First, identify which column contains the student ID\n",
        "    id_columns = ['student_id', 'stu_id', 'id']\n",
        "    id_col = None\n",
        "\n",
        "    for col in id_columns:\n",
        "        if col in df.columns:\n",
        "            id_col = col\n",
        "            break\n",
        "\n",
        "    # If no ID column is found, check if there's a 'Name' column (for quiz_results.csv)\n",
        "    if id_col is None:\n",
        "        if 'Name' in df.columns:\n",
        "            # We'll handle this separately for now\n",
        "            return df\n",
        "        else:\n",
        "            print(f\"Warning: No ID column found in dataframe with columns: {df.columns}\")\n",
        "            return df\n",
        "\n",
        "    # Make a copy of the dataframe to avoid the SettingWithCopyWarning\n",
        "    df = df.copy()\n",
        "\n",
        "    # Ensure ID is string type\n",
        "    df[id_col] = df[id_col].astype(str)\n",
        "\n",
        "    # Remove prefixes and standardize format\n",
        "    df[id_col] = df[id_col].str.replace('S-', '')\n",
        "    df[id_col] = df[id_col].str.replace('ID', '')\n",
        "    df[id_col] = df[id_col].str.strip()\n",
        "\n",
        "    # Rename the column to the standard 'student_id'\n",
        "    if id_col != 'student_id':\n",
        "        df = df.rename(columns={id_col: 'student_id'})\n",
        "\n",
        "    return df\n",
        "\n",
        "# Clean timestamps\n",
        "def clean_timestamps(df):\n",
        "    # Make a copy of the dataframe\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert timestamp columns to datetime\n",
        "    time_cols = [col for col in df.columns\n",
        "                if any(term in col.lower() for term in ['time', 'date', 'login'])]\n",
        "\n",
        "    for col in time_cols:\n",
        "        if col in df.columns:  # Verify column exists\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply cleaning functions one at a time with error handling\n",
        "clean_dfs = []\n",
        "for i, df in enumerate(dfs):\n",
        "    try:\n",
        "        # Print which file we're processing\n",
        "        print(f\"Cleaning file: {os.path.basename(data_files[i])}\")\n",
        "\n",
        "        # Apply cleaning functions\n",
        "        cleaned = clean_student_id(df)\n",
        "        cleaned = clean_timestamps(cleaned)\n",
        "\n",
        "        clean_dfs.append(cleaned)\n",
        "    except Exception as e:\n",
        "        print(f\"Error cleaning {os.path.basename(data_files[i])}: {e}\")\n",
        "        # Add the original df to maintain indexing\n",
        "        clean_dfs.append(df)\n",
        "    print(\"======\")\n",
        "\n",
        "for df in clean_dfs:\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    print(df.head(3))\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC7o5_nnjQTa",
        "outputId": "7550d1dd-3c61-4db7-8a6f-aa37676a5cca"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After: ['Name', 'Quiz ID', 'Time Taken', 'Mark', 'Status']\n",
            "After: ['student_id', 'first_name', 'last_name', 'district', 'registration_date', 'status']\n",
            "After: ['assignment_id', 'title', 'due_date', 'max_points', 'weight', 'category']\n",
            "After: ['student_id', 'first_login', 'last_login', 'logins_count', 'avg_session_minutes', 'device_type']\n",
            "After: ['student_id', 'assignment_id', 'score', 'submission_time', 'feedback']\n",
            "Shape: (32, 5)\n",
            "Columns: ['Name', 'Quiz ID', 'Time Taken', 'Mark', 'Status']\n",
            "               Name Quiz ID           Time Taken  Mark    Status\n",
            "0  Katniss Everdeen    Q101  2022-09-20 10:15:00    45  Complete\n",
            "1     Peeta Mellark    Q101  2022-09-20 10:30:00    42  Complete\n",
            "2    Gale Hawthorne    Q101  2022-09-20 10:45:00    38  Complete\n",
            "---\n",
            "Shape: (16, 6)\n",
            "Columns: ['student_id', 'first_name', 'last_name', 'district', 'registration_date', 'status']\n",
            "  student_id first_name  last_name district registration_date  status\n",
            "0      S-001    Katniss   Everdeen       12        2022-09-01  Active\n",
            "1      S-002      Peeta    Mellark       12        2022-09-01  Active\n",
            "2      ID003       Gale  Hawthorne       12          9/2/2022  active\n",
            "---\n",
            "Shape: (9, 6)\n",
            "Columns: ['assignment_id', 'title', 'due_date', 'max_points', 'weight', 'category']\n",
            "  assignment_id                     title    due_date  max_points weight  \\\n",
            "0          A101      Data Analysis Basics  2022-10-15         100    15%   \n",
            "1          A102       Statistical Methods  2022-10-30         100    20%   \n",
            "2          A103  Visualization Techniques  2022-11-15         100    15%   \n",
            "\n",
            "     category  \n",
            "0  Assignment  \n",
            "1  Assignment  \n",
            "2  Assignment  \n",
            "---\n",
            "Shape: (16, 6)\n",
            "Columns: ['student_id', 'first_login', 'last_login', 'logins_count', 'avg_session_minutes', 'device_type']\n",
            "  student_id          first_login           last_login  logins_count  \\\n",
            "0      S-001  2022-09-01 08:30:25  2022-10-20 16:45:12            87   \n",
            "1      S-002  2022-09-01 09:15:43  2022-10-20 15:20:18            65   \n",
            "2      ID003  01/09/2022 10:45:12  20/10/2022 17:30:45            42   \n",
            "\n",
            "   avg_session_minutes device_type  \n",
            "0                 42.3      laptop  \n",
            "1                 38.9      laptop  \n",
            "2                 25.7      mobile  \n",
            "---\n",
            "Shape: (16, 5)\n",
            "Columns: ['student_id', 'assignment_id', 'score', 'submission_time', 'feedback']\n",
            "  student_id assignment_id  score      submission_time  \\\n",
            "0        001          A101   87.5  2022-10-15 14:30:00   \n",
            "1        002          A101   92.3  2022-10-15 09:45:00   \n",
            "2        003          A101   76.8  2022-10-15 23:59:59   \n",
            "\n",
            "                               feedback  \n",
            "0  Good analysis but missing conclusion  \n",
            "1                        Excellent work  \n",
            "2                     Needs more detail  \n",
            "---\n",
            "Cleaning file: additional-quiz-results.csv\n",
            "======\n",
            "Cleaning file: student-records.csv\n",
            "======\n",
            "Cleaning file: course-assignments.csv\n",
            "Warning: No ID column found in dataframe with columns: Index(['assignment_id', 'title', 'due_date', 'max_points', 'weight',\n",
            "       'category'],\n",
            "      dtype='object')\n",
            "======\n",
            "Cleaning file: login-activity.csv\n",
            "======\n",
            "Cleaning file: assessment-scores.csv\n",
            "======\n",
            "Shape: (32, 5)\n",
            "Columns: ['Name', 'Quiz ID', 'Time Taken', 'Mark', 'Status']\n",
            "               Name Quiz ID          Time Taken  Mark    Status\n",
            "0  Katniss Everdeen    Q101 2022-09-20 10:15:00    45  Complete\n",
            "1     Peeta Mellark    Q101 2022-09-20 10:30:00    42  Complete\n",
            "2    Gale Hawthorne    Q101 2022-09-20 10:45:00    38  Complete\n",
            "---\n",
            "Shape: (16, 6)\n",
            "Columns: ['student_id', 'first_name', 'last_name', 'district', 'registration_date', 'status']\n",
            "  student_id first_name  last_name district registration_date  status\n",
            "0        001    Katniss   Everdeen       12        2022-09-01  Active\n",
            "1        002      Peeta    Mellark       12        2022-09-01  Active\n",
            "2        003       Gale  Hawthorne       12               NaT  active\n",
            "---\n",
            "Shape: (9, 6)\n",
            "Columns: ['assignment_id', 'title', 'due_date', 'max_points', 'weight', 'category']\n",
            "  assignment_id                     title   due_date  max_points weight  \\\n",
            "0          A101      Data Analysis Basics 2022-10-15         100    15%   \n",
            "1          A102       Statistical Methods 2022-10-30         100    20%   \n",
            "2          A103  Visualization Techniques 2022-11-15         100    15%   \n",
            "\n",
            "     category  \n",
            "0  Assignment  \n",
            "1  Assignment  \n",
            "2  Assignment  \n",
            "---\n",
            "Shape: (16, 6)\n",
            "Columns: ['student_id', 'first_login', 'last_login', 'logins_count', 'avg_session_minutes', 'device_type']\n",
            "  student_id         first_login          last_login  \\\n",
            "0        001 2022-09-01 08:30:25 2022-10-20 16:45:12   \n",
            "1        002 2022-09-01 09:15:43 2022-10-20 15:20:18   \n",
            "2        003                 NaT                 NaT   \n",
            "\n",
            "                   logins_count  avg_session_minutes device_type  \n",
            "0 1970-01-01 00:00:00.000000087                 42.3      laptop  \n",
            "1 1970-01-01 00:00:00.000000065                 38.9      laptop  \n",
            "2 1970-01-01 00:00:00.000000042                 25.7      mobile  \n",
            "---\n",
            "Shape: (16, 5)\n",
            "Columns: ['student_id', 'assignment_id', 'score', 'submission_time', 'feedback']\n",
            "  student_id assignment_id  score     submission_time  \\\n",
            "0        001          A101   87.5 2022-10-15 14:30:00   \n",
            "1        002          A101   92.3 2022-10-15 09:45:00   \n",
            "2        003          A101   76.8 2022-10-15 23:59:59   \n",
            "\n",
            "                               feedback  \n",
            "0  Good analysis but missing conclusion  \n",
            "1                        Excellent work  \n",
            "2                     Needs more detail  \n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enriching"
      ],
      "metadata": {
        "id": "k-zM-6fPjn7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Special handling for quiz_results.csv which uses names instead of IDs\n",
        "quiz_df = None\n",
        "for i, df in enumerate(clean_dfs):\n",
        "    if 'Name' in df.columns and 'Mark' in df.columns:\n",
        "        quiz_df = df.copy()\n",
        "\n",
        "        # Split the Name column into first_name and last_name\n",
        "        quiz_df[['first_name', 'last_name']] = quiz_df['Name'].str.split(' ', n=1, expand=True)\n",
        "\n",
        "        # Rename Mark to score for consistency\n",
        "        quiz_df = quiz_df.rename(columns={'Mark': 'score', 'Quiz ID': 'assignment_id'})\n",
        "\n",
        "        # Remove this from clean_dfs as we'll handle it separately\n",
        "        clean_dfs[i] = None\n",
        "\n",
        "# Remove None entries from clean_dfs\n",
        "clean_dfs = [df for df in clean_dfs if df is not None]\n",
        "\n",
        "# Get the student records dataframe which has first_name and last_name\n",
        "student_records_df = None\n",
        "for df in clean_dfs:\n",
        "    if 'first_name' in df.columns and 'last_name' in df.columns:\n",
        "        student_records_df = df\n",
        "        break\n",
        "\n",
        "# If we have both quiz_df and student_records_df, we can match the quiz results to student IDs\n",
        "if quiz_df is not None and student_records_df is not None:\n",
        "    # Merge on first_name and last_name\n",
        "    quiz_df = pd.merge(\n",
        "        quiz_df,\n",
        "        student_records_df[['student_id', 'first_name', 'last_name']],\n",
        "        on=['first_name', 'last_name'],\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Add this back to clean_dfs\n",
        "    clean_dfs.append(quiz_df)\n",
        "\n",
        "# Start with the student records as our base\n",
        "base_df = student_records_df if student_records_df is not None else clean_dfs[0]\n",
        "merged_df = base_df.copy()\n",
        "\n",
        "# Merge the other dataframes one by one\n",
        "for df in clean_dfs:\n",
        "    if df is not base_df and 'student_id' in df.columns:  # Skip base_df and ensure student_id exists\n",
        "        try:\n",
        "            merged_df = pd.merge(\n",
        "                merged_df, df,\n",
        "                on='student_id',\n",
        "                how='outer',\n",
        "                suffixes=('', '_drop')\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging dataframe: {e}\")\n",
        "            print(f\"Columns in first df: {merged_df.columns.tolist()}\")\n",
        "            print(f\"Columns in second df: {df.columns.tolist()}\")\n",
        "\n",
        "# Remove redundant columns\n",
        "merged_df = merged_df.loc[:, ~merged_df.columns.str.endswith('_drop')]\n",
        "\n",
        "# Create derived features with error handling\n",
        "try:\n",
        "    if 'last_login' in merged_df.columns and 'first_login' in merged_df.columns:\n",
        "        # Ensure both columns are datetime type before subtraction\n",
        "        if pd.api.types.is_datetime64_any_dtype(merged_df['last_login']) and pd.api.types.is_datetime64_any_dtype(merged_df['first_login']):\n",
        "            # Calculate the difference and convert to days as integer\n",
        "            merged_df['days_active'] = (merged_df['last_login'] - merged_df['first_login']).dt.total_seconds() / 86400\n",
        "            merged_df['days_active'] = merged_df['days_active'].astype('float').round(1)\n",
        "\n",
        "            if 'logins_count' in merged_df.columns:\n",
        "                # Convert logins_count to numeric if needed\n",
        "                if not pd.api.types.is_numeric_dtype(merged_df['logins_count']):\n",
        "                    merged_df['logins_count'] = pd.to_numeric(merged_df['logins_count'], errors='coerce')\n",
        "\n",
        "                # Now calculate engagement score\n",
        "                merged_df['engagement_score'] = merged_df['logins_count'] / merged_df['days_active'].clip(lower=1)\n",
        "\n",
        "    if 'score' in merged_df.columns:\n",
        "        merged_df['performance_category'] = pd.cut(\n",
        "            merged_df['score'],\n",
        "            bins=[0, 60, 70, 80, 90, 100],\n",
        "            labels=['F', 'D', 'C', 'B', 'A']\n",
        "        )\n",
        "except Exception as e:\n",
        "    print(f\"Error creating derived features: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mcb9mB1Yj2Sd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validating"
      ],
      "metadata": {
        "id": "K6mFLdRuj2-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Consistency checks\n",
        "def validate_data(df):\n",
        "    validation_issues = []\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    df = df.copy()\n",
        "\n",
        "    # Check value ranges for score column\n",
        "    if 'score' in df.columns:\n",
        "        # Convert score to numeric if it's not already\n",
        "        df['score'] = pd.to_numeric(df['score'], errors='coerce')\n",
        "\n",
        "        valid_score_range = (df['score'] >= 0) & (df['score'] <= 100) | df['score'].isna()\n",
        "        if not valid_score_range.all():\n",
        "            invalid_count = (~valid_score_range).sum()\n",
        "            validation_issues.append(f\"Warning: {invalid_count} scores outside valid range (0-100)\")\n",
        "\n",
        "    # Check for logical consistency in login dates\n",
        "    if 'last_login' in df.columns and 'first_login' in df.columns:\n",
        "        # Only check rows that have both dates\n",
        "        valid_rows = df['last_login'].notna() & df['first_login'].notna()\n",
        "\n",
        "        if valid_rows.any():\n",
        "            login_consistent = (df.loc[valid_rows, 'last_login'] >= df.loc[valid_rows, 'first_login']).all()\n",
        "            if not login_consistent:\n",
        "                inconsistent_count = (~(df.loc[valid_rows, 'last_login'] >= df.loc[valid_rows, 'first_login'])).sum()\n",
        "                validation_issues.append(f\"Warning: {inconsistent_count} rows have last_login earlier than first_login\")\n",
        "\n",
        "                # Identify these rows\n",
        "                inconsistent_rows = df.loc[valid_rows & ~(df['last_login'] >= df['first_login'])]\n",
        "                print(\"Inconsistent login date rows:\")\n",
        "                print(inconsistent_rows[['student_id', 'first_login', 'last_login']])\n",
        "\n",
        "                # Fix the issue by swapping dates\n",
        "                for idx in inconsistent_rows.index:\n",
        "                    df.loc[idx, 'first_login'], df.loc[idx, 'last_login'] = df.loc[idx, 'last_login'], df.loc[idx, 'first_login']\n",
        "                print(\"Dates swapped to fix inconsistency\")\n",
        "\n",
        "    # Check for duplicates if we have student_id and assignment_id\n",
        "    if 'student_id' in df.columns and 'assignment_id' in df.columns:\n",
        "        duplicate_entries = df.duplicated(subset=['student_id', 'assignment_id'], keep='first')\n",
        "        if duplicate_entries.any():\n",
        "            dup_count = duplicate_entries.sum()\n",
        "            validation_issues.append(f\"Warning: {dup_count} duplicate entries found\")\n",
        "\n",
        "            # Keep only the first occurrence of each student_id/assignment_id pair\n",
        "            df = df.drop_duplicates(subset=['student_id', 'assignment_id'], keep='first')\n",
        "            print(f\"Removed {dup_count} duplicate entries\")\n",
        "\n",
        "    # Check for missing critical data\n",
        "    if 'student_id' in df.columns:\n",
        "        critical_cols = ['student_id']\n",
        "        if 'score' in df.columns:\n",
        "            critical_cols.append('score')\n",
        "        if 'assignment_id' in df.columns:\n",
        "            critical_cols.append('assignment_id')\n",
        "\n",
        "        missing_critical = df[critical_cols].isnull().any(axis=1)\n",
        "        if missing_critical.any():\n",
        "            missing_count = missing_critical.sum()\n",
        "            validation_issues.append(f\"Warning: {missing_count} rows with missing critical data\")\n",
        "\n",
        "            # Identify these rows\n",
        "            print(\"Rows with missing critical data:\")\n",
        "            print(df.loc[missing_critical, critical_cols])\n",
        "\n",
        "    # Print validation summary\n",
        "    if validation_issues:\n",
        "        print(\"Validation Summary:\")\n",
        "        for issue in validation_issues:\n",
        "            print(f\"- {issue}\")\n",
        "    else:\n",
        "        print(\"All validation checks passed!\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Apply validation and get the final dataset\n",
        "try:\n",
        "    final_df = validate_data(merged_df)\n",
        "    print(f\"Final dataset shape: {final_df.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during validation: {e}\")\n",
        "    print(\"Using merged_df without validation\")\n",
        "    final_df = merged_df\n",
        "\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5lB7InCj1un",
        "outputId": "85251430-e793-4622-eb9c-42a9c87515fb"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 16 duplicate entries\n",
            "Rows with missing critical data:\n",
            "   student_id  score assignment_id\n",
            "16        009    NaN          A101\n",
            "31        nan    NaN           NaN\n",
            "Validation Summary:\n",
            "- Warning: 16 duplicate entries found\n",
            "- Warning: 2 rows with missing critical data\n",
            "Final dataset shape: (17, 21)\n",
            "                 Name assignment_id          Time Taken  score    Status  \\\n",
            "0    Katniss Everdeen          Q101 2022-09-20 10:15:00     45  Complete   \n",
            "1       Peeta Mellark          Q101 2022-09-20 10:30:00     42  Complete   \n",
            "2      Gale Hawthorne          Q101 2022-09-20 10:45:00     38  Complete   \n",
            "3   Primrose Everdeen          Q101 2022-09-20 11:00:00     47  Complete   \n",
            "4  Haymitch Abernathy          Q101 2022-09-20 11:15:00     35  Complete   \n",
            "5       Effie Trinket          Q101 2022-09-20 11:30:00     48  Complete   \n",
            "6               Cinna          Q101 2022-09-20 11:45:00     43  Complete   \n",
            "7                 Rue          Q101 2022-09-20 12:00:00     41  Complete   \n",
            "8              Thresh          Q101 2022-09-20 12:15:00     39  Complete   \n",
            "9               Clove          Q101 2022-09-20 12:30:00     44  Complete   \n",
            "\n",
            "  first_name  last_name student_id  \n",
            "0    Katniss   Everdeen        001  \n",
            "1      Peeta    Mellark        002  \n",
            "2       Gale  Hawthorne        003  \n",
            "3   Primrose   Everdeen        004  \n",
            "4   Haymitch  Abernathy        005  \n",
            "5      Effie    Trinket        006  \n",
            "6      Cinna       None        007  \n",
            "7        Rue       None        008  \n",
            "8     Thresh       None        009  \n",
            "9      Clove       None        010  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Publishing"
      ],
      "metadata": {
        "id": "zexHIXYLkcLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define output directory\n",
        "output_dir = './cleaned_data/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "try:\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    print(f\"Output directory created/verified: {output_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"Unable to create output directory: {str(e)}\")\n",
        "    # Default to current directory if output_dir creation fails\n",
        "    output_dir = './'\n",
        "\n",
        "# Save clean dataset\n",
        "try:\n",
        "    output_file = os.path.join(output_dir, 'clean_student_performance.csv')\n",
        "    final_df.to_csv(output_file, index=False)\n",
        "    print(f\"Clean dataset saved to: {output_file}\")\n",
        "\n",
        "    # Show a sample of the cleaned data\n",
        "    print(\"\\nSample of cleaned data:\")\n",
        "    print(final_df.head(3))\n",
        "except Exception as e:\n",
        "    print(f\"Error saving clean dataset: {str(e)}\")\n",
        "\n",
        "# Create data dictionary\n",
        "try:\n",
        "    data_dict = pd.DataFrame({\n",
        "        'column': final_df.columns,\n",
        "        'data_type': final_df.dtypes.astype(str),\n",
        "        'missing_values': final_df.isnull().sum(),\n",
        "        'unique_values': [final_df[col].nunique() for col in final_df.columns],\n",
        "        'sample_values': [str(final_df[col].dropna().sample(n=min(3, final_df[col].nunique())).tolist())[:50] + '...'\n",
        "                         if not final_df[col].empty else 'No samples'\n",
        "                         for col in final_df.columns]\n",
        "    })\n",
        "\n",
        "    dict_file = os.path.join(output_dir, 'data_dictionary.csv')\n",
        "    data_dict.to_csv(dict_file, index=False)\n",
        "    print(f\"\\nData dictionary saved to: {dict_file}\")\n",
        "\n",
        "    # Show the data dictionary\n",
        "    print(\"\\nData Dictionary Preview:\")\n",
        "    pd.set_option('display.max_colwidth', 30)\n",
        "    print(data_dict.head())\n",
        "    pd.reset_option('display.max_colwidth')\n",
        "except Exception as e:\n",
        "    print(f\"Error creating data dictionary: {str(e)}\")\n",
        "\n",
        "# Documentation of transformation process\n",
        "try:\n",
        "    log_file = os.path.join(output_dir, 'transformation_log.txt')\n",
        "    with open(log_file, 'w') as f:\n",
        "        f.write(\"Data Wrangling Process\\n\")\n",
        "        f.write(\"======================\\n\\n\")\n",
        "\n",
        "        f.write(\"1. Data Sources:\\n\")\n",
        "        for file in data_files:\n",
        "            f.write(f\"   - {os.path.basename(file)}\\n\")\n",
        "\n",
        "        f.write(\"\\n2. Transformations Applied:\\n\")\n",
        "        f.write(\"   - Standardized student IDs across all sources\\n\")\n",
        "        f.write(\"   - Converted all timestamps to consistent datetime format\\n\")\n",
        "        f.write(\"   - Handled missing values in critical fields\\n\")\n",
        "        f.write(\"   - Merged data from multiple sources\\n\")\n",
        "        f.write(\"   - Created derived metrics (days_active, engagement_score, performance_category)\\n\")\n",
        "        f.write(\"   - Fixed inconsistencies and removed duplicates\\n\")\n",
        "\n",
        "        f.write(\"\\n3. Final Dataset Statistics:\\n\")\n",
        "        f.write(f\"   - Total records: {len(final_df)}\\n\")\n",
        "        f.write(f\"   - Total columns: {len(final_df.columns)}\\n\")\n",
        "\n",
        "        f.write(\"\\n4. Known Issues:\\n\")\n",
        "        f.write(\"   - Some students may have missing assessment data\\n\")\n",
        "        f.write(\"   - Login activity might be incomplete for some students\\n\")\n",
        "\n",
        "    print(f\"\\nTransformation log saved to: {log_file}\")\n",
        "\n",
        "    # Display the log content\n",
        "    print(\"\\nTransformation Log Preview:\")\n",
        "    with open(log_file, 'r') as f:\n",
        "        print(f.read()[:500] + \"...\\n(Log truncated for display)\")\n",
        "except Exception as e:\n",
        "    print(f\"Error creating transformation log: {str(e)}\")\n",
        "\n",
        "print(\"\\nData publishing complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQuKpdiJkdUM",
        "outputId": "997acaee-a383-48ae-90a3-e1174db70df9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output directory created/verified: ./cleaned_data/\n",
            "Clean dataset saved to: ./cleaned_data/clean_student_performance.csv\n",
            "\n",
            "Sample of cleaned data:\n",
            "  student_id first_name  last_name district registration_date  status  \\\n",
            "0        001    Katniss   Everdeen       12        2022-09-01  Active   \n",
            "2        002      Peeta    Mellark       12        2022-09-01  Active   \n",
            "4        003       Gale  Hawthorne       12               NaT  active   \n",
            "\n",
            "          first_login          last_login  logins_count  avg_session_minutes  \\\n",
            "0 2022-09-01 08:30:25 2022-10-20 16:45:12            87                 42.3   \n",
            "2 2022-09-01 09:15:43 2022-10-20 15:20:18            65                 38.9   \n",
            "4                 NaT                 NaT            42                 25.7   \n",
            "\n",
            "   ... assignment_id score     submission_time  \\\n",
            "0  ...          A101  87.5 2022-10-15 14:30:00   \n",
            "2  ...          A101  92.3 2022-10-15 09:45:00   \n",
            "4  ...          A101  76.8 2022-10-15 23:59:59   \n",
            "\n",
            "                               feedback              Name          Time Taken  \\\n",
            "0  Good analysis but missing conclusion  Katniss Everdeen 2022-09-20 10:15:00   \n",
            "2                        Excellent work     Peeta Mellark 2022-09-20 10:30:00   \n",
            "4                     Needs more detail    Gale Hawthorne 2022-09-20 10:45:00   \n",
            "\n",
            "     Status days_active  engagement_score  performance_category  \n",
            "0  Complete        49.3          1.764706                     B  \n",
            "2  Complete        49.3          1.318458                     A  \n",
            "4  Complete         NaN               NaN                     C  \n",
            "\n",
            "[3 rows x 21 columns]\n",
            "\n",
            "Data dictionary saved to: ./cleaned_data/data_dictionary.csv\n",
            "\n",
            "Data Dictionary Preview:\n",
            "                              column       data_type  missing_values  \\\n",
            "student_id                student_id          object               0   \n",
            "first_name                first_name          object               1   \n",
            "last_name                  last_name          object               7   \n",
            "district                    district          object               1   \n",
            "registration_date  registration_date  datetime64[ns]               7   \n",
            "\n",
            "                   unique_values                  sample_values  \n",
            "student_id                    17       ['nan', '005', '006']...  \n",
            "first_name                    16  ['Gale', 'Effie', 'Beetee'...  \n",
            "last_name                      9  ['Hawthorne', 'Cresta', 'M...  \n",
            "district                       8       ['7', '2', 'Capitol']...  \n",
            "registration_date              6  [Timestamp('2022-09-01 00:...  \n",
            "\n",
            "Transformation log saved to: ./cleaned_data/transformation_log.txt\n",
            "\n",
            "Transformation Log Preview:\n",
            "Data Wrangling Process\n",
            "======================\n",
            "\n",
            "1. Data Sources:\n",
            "   - additional-quiz-results.csv\n",
            "   - student-records.csv\n",
            "   - course-assignments.csv\n",
            "   - login-activity.csv\n",
            "   - assessment-scores.csv\n",
            "\n",
            "2. Transformations Applied:\n",
            "   - Standardized student IDs across all sources\n",
            "   - Converted all timestamps to consistent datetime format\n",
            "   - Handled missing values in critical fields\n",
            "   - Merged data from multiple sources\n",
            "   - Created derived metrics (days_active, engagement_score, performance_ca...\n",
            "(Log truncated for display)\n",
            "\n",
            "Data publishing complete!\n"
          ]
        }
      ]
    }
  ]
}